{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b83214",
   "metadata": {},
   "source": [
    "## Import and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91503da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "heart_df = pd.read_csv('dataset/heart.csv')\n",
    "print(heart_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f5dc1",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561261f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age               0\n",
       "Sex               0\n",
       "ChestPainType     0\n",
       "RestingBP         0\n",
       "Cholesterol       0\n",
       "FastingBS         0\n",
       "RestingECG        0\n",
       "MaxHR             0\n",
       "ExerciseAngina    0\n",
       "Oldpeak           0\n",
       "ST_Slope          0\n",
       "HeartDisease      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d974da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2aa58e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                50\n",
       "Sex                 2\n",
       "ChestPainType       4\n",
       "RestingBP          67\n",
       "Cholesterol       222\n",
       "FastingBS           2\n",
       "RestingECG          3\n",
       "MaxHR             119\n",
       "ExerciseAngina      2\n",
       "Oldpeak            53\n",
       "ST_Slope            3\n",
       "HeartDisease        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f5555f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = heart_df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f9cc2",
   "metadata": {},
   "source": [
    "### converting categorical variables into numeric\n",
    "- Sex: M=0 , F=1\n",
    "- ChecstPainType: ATA= 0, NAP=1, ASY=2, TA=3\n",
    "- RestingECG: Normal=0, ST=1, LVH=2\n",
    "- ExerciseAngina: N=0, Y=1\n",
    "- ST_Slope: Up=0, Flat=1, Down=2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a2afd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex\n",
      "['M' 'F'] [0, 1]\n",
      "******************************************************************************************\n",
      "\n",
      "ChestPainType\n",
      "['ATA' 'NAP' 'ASY' 'TA'] [0, 1, 2, 3]\n",
      "******************************************************************************************\n",
      "\n",
      "RestingECG\n",
      "['Normal' 'ST' 'LVH'] [0, 1, 2]\n",
      "******************************************************************************************\n",
      "\n",
      "ExerciseAngina\n",
      "['N' 'Y'] [0, 1]\n",
      "******************************************************************************************\n",
      "\n",
      "ST_Slope\n",
      "['Up' 'Flat' 'Down'] [0, 1, 2]\n",
      "******************************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/d20r98cx4w1bjlcvz3ygtl1r0000gn/T/ipykernel_6776/2627456545.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  heart_df[col] = heart_df[col].replace((heart_df[col].unique()), range(heart_df[col].nunique()))\n"
     ]
    }
   ],
   "source": [
    "for col in cat_col:\n",
    "    print(col)\n",
    "    print((heart_df[col].unique()), list(range(heart_df[col].nunique())))\n",
    "    heart_df[col] = heart_df[col].replace((heart_df[col].unique()), range(heart_df[col].nunique()))\n",
    "    print('*'*90)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33306d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cholesterol\n",
       "0      172\n",
       "254     11\n",
       "223     10\n",
       "220     10\n",
       "230      9\n",
       "      ... \n",
       "392      1\n",
       "316      1\n",
       "153      1\n",
       "466      1\n",
       "131      1\n",
       "Name: count, Length: 222, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df['Cholesterol'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060da35",
   "metadata": {},
   "source": [
    "Imputing the 0 values in cholesterol column with KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "046fa1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df['Cholesterol'] = heart_df['Cholesterol'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8ac033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/bertrandmutangana/Library/Python/3.11/lib/python/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/bertrandmutangana/Library/Python/3.11/lib/python/site-packages (from scikit-learn) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/bertrandmutangana/Library/Python/3.11/lib/python/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/bertrandmutangana/Library/Python/3.11/lib/python/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/bertrandmutangana/Library/Python/3.11/lib/python/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7781efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "after_impute = imputer.fit_transform(heart_df)\n",
    "heart_df = pd.DataFrame(after_impute, columns=heart_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25dcd11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df[\"Cholesterol\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7dbbc6",
   "metadata": {},
   "source": [
    "### Doing the same for resting blood pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea713b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "heart_df['RestingBP'] = heart_df['RestingBP'].replace(0, np.nan)\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "after_impute = imputer.fit_transform(heart_df)\n",
    "heart_df = pd.DataFrame(after_impute, columns=heart_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1af5a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df[\"RestingBP\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f2adb",
   "metadata": {},
   "source": [
    "### Change columns type to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b27eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutOldPeak = heart_df.columns\n",
    "withoutOldPeak = withoutOldPeak.drop('Oldpeak')\n",
    "heart_df[withoutOldPeak] = heart_df[withoutOldPeak].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1f0888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly in /Users/bertrandmutangana/Library/Python/3.11/lib/python/site-packages (6.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/bertrandmutangana/Library/Python/3.11/lib/python/site-packages (from plotly) (1.48.0)\n",
      "Requirement already satisfied: packaging in /Users/bertrandmutangana/Library/Python/3.11/lib/python/site-packages (from plotly) (25.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a01c8a",
   "metadata": {},
   "source": [
    "## Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba52a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    heart_df.drop('HeartDisease', axis=1),\n",
    "    heart_df['HeartDisease'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=heart_df['HeartDisease']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a0491",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743b7d6",
   "metadata": {},
   "source": [
    "### Logistic regression (Great for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "858382f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression score: 0.8586956521739131\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features first\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Perform hyper parameter tuning to find the best solver for a Logistic Regression model\n",
    "solver = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "best_solver = ''\n",
    "test_score = np.zeros(6) # Array to store each solver's score\n",
    "\n",
    "for i, n in enumerate(solver):\n",
    "    lr = LogisticRegression(solver=n, max_iter=1000).fit(X_train_scaled, y_train)  # Increased max_iter\n",
    "    test_score[i] = lr.score(X_test_scaled, y_test)\n",
    "    if lr.score(X_test_scaled, y_test) == test_score.max():\n",
    "        best_solver = n\n",
    "\n",
    "lr = LogisticRegression(solver=best_solver, max_iter=1000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr.predict(X_test_scaled)\n",
    "print(f'Logistic regression score: {accuracy_score(y_test, lr_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680d852",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5070e2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM f1_score kernel(linear): 0.8422922535440344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "kernels = {'linear': 0, 'poly': 0, 'rbf': 0, 'sigmoid': 0}\n",
    "best = ''\n",
    "\n",
    "for i in kernels:\n",
    "    svm = SVC(kernel=i)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_hat = svm.predict(X_test)\n",
    "    kernels[i] = f1_score(y_test, y_hat, average=\"weighted\")\n",
    "    if kernels[i] == max(kernels.values()):\n",
    "        best = i\n",
    "\n",
    "svm = SVC(kernel=best)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "print(f\"SVM f1_score kernel({best}): {f1_score(y_test, svm_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24143209",
   "metadata": {},
   "source": [
    "### Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef364749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTrees's Accuracy:  0.8097826086956522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dtree = DecisionTreeClassifier(class_weight='balanced')\n",
    "param_grid = {\n",
    "    'max_depth': [3,4,5,6,7,8],\n",
    "    'min_samples_split': [2,3,4],\n",
    "    'min_samples_leaf': [1,2,3,4],\n",
    "    'random_state': [0,42]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(dtree, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "Ctree = DecisionTreeClassifier(**grid_search.best_params_, class_weight='balanced')\n",
    "Ctree.fit(X_train, y_train)\n",
    "dtc_pred = Ctree.predict(X_test)\n",
    "print(\"DecisionTrees's Accuracy: \", accuracy_score(y_test, dtc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace5d30",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9452c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier's accuracy:  0.8478260869565217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 500],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [3,5,9,19],\n",
    "    'max_leaf_nodes': [3,6,9]\n",
    "}\n",
    "grid_search = GridSearchCV(rfc, param_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "rfctree = RandomForestClassifier(**grid_search.best_params_)\n",
    "rfctree.fit(X_train, y_train)\n",
    "rfc_pred = rfctree.predict(X_test)\n",
    "print(\"Random forest classifier's accuracy: \", accuracy_score(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf40fea",
   "metadata": {},
   "source": [
    "## Dumping and Loading the model via Pickle object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e962e54",
   "metadata": {},
   "source": [
    "### Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6541bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file size: 761 bytes\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Save the model properly\n",
    "with open('LR_model.pkl', 'wb') as file:\n",
    "    pickle.dump(lr, file)\n",
    "\n",
    "# Check file size\n",
    "file_size = os.path.getsize('LogisticRegression_model.pkl')\n",
    "print(f\"Pickle file size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae02085",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file size: 31537 bytes\n"
     ]
    }
   ],
   "source": [
    "# Save the model properly\n",
    "with open('SVM_model.pkl', 'wb') as file:\n",
    "    pickle.dump(svm, file)\n",
    "\n",
    "# Check file size\n",
    "file_size = os.path.getsize('SVM_model.pkl')\n",
    "print(f\"Pickle file size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363818f",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071a9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file size: 2588 bytes\n"
     ]
    }
   ],
   "source": [
    "# Save the model properly\n",
    "with open('DTC_model.pkl', 'wb') as file:\n",
    "    pickle.dump(Ctree, file)\n",
    "\n",
    "# Check file size\n",
    "file_size = os.path.getsize('DTC_model.pkl')\n",
    "print(f\"Pickle file size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f86a92d",
   "metadata": {},
   "source": [
    "### Random Forest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d108dc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file size: 119892 bytes\n"
     ]
    }
   ],
   "source": [
    "# Save the model properly\n",
    "with open('RFC_model.pkl', 'wb') as file:\n",
    "    pickle.dump(rfctree, file)\n",
    "\n",
    "# Check file size\n",
    "file_size = os.path.getsize('RFC_model.pkl')\n",
    "print(f\"Pickle file size: {file_size} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
